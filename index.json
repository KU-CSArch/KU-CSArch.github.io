[{"authors":null,"categories":null,"content":"Gunjae Koo is an assistant professor in the Department of Computer Science and Engineering at Korea University. His research interest is in computer architecture, memory systems, storage systems, and accelerator design. His current research focuses on memory systems of parallel processors and near data processing on storage platforms.\nGunjae completed his Ph.D. in the Department of Electrical Engineering at the University of Southern California (USC) working with Professor Murali Annavaram. He earned his B.S. and M.S. degrees in Electrical and Computer Engineering at Seoul National University, South Korea. Before joining Korea University, he was an assistant professor at Hongik University. He worked for LG Electronics Digital TV and System IC laboratory as a senior research engineer involved in multiple research projects developing SoCs for storage devices and digital TVs. He also worked on the memory controller architecture for high-performance server systems at Intel.\n","date":1647388800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1647388800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://ku-csarch.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Gunjae Koo is an assistant professor in the Department of Computer Science and Engineering at Korea University. His research interest is in computer architecture, memory systems, storage systems, and accelerator design.","tags":null,"title":"Gunjae Koo","type":"authors"},{"authors":["jongmin-lee"],"categories":null,"content":"Jongmin Lee is a PhD student in the Department of Computer Science and Engineering at Korea University working with Professor Gunjae Koo. His research interests lie in computer architecture and trusted computing. His current research focuses on secure architecture against transient execution attacks.\nJongmin Lee earned his B.S and M.S degrees in Computer Engineering at Korea University, South Korea. He worked for TMAX OS as a senior reseacher invloved in multiple projects developing operating systems and software frameworks.\n","date":1647388800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1647388800,"objectID":"40ff7f58a64175cfed8595b51741ac94","permalink":"https://ku-csarch.github.io/authors/jongmin-lee/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jongmin-lee/","section":"authors","summary":"Jongmin Lee is a PhD student in the Department of Computer Science and Engineering at Korea University working with Professor Gunjae Koo. His research interests lie in computer architecture and trusted computing.","tags":null,"title":"Jongmin Lee","type":"authors"},{"authors":["inje-kim"],"categories":null,"content":"Inje Kim is a graduate student pursuing a master’s degree in the Department of Computer Science and Engineering at Korea University. His research interest is in accelerator architecture, GPGPU, graph neural networks (GNNs). His current research focuses on characterizing graph neural networks on GPGPU and acceleration of graph neural networks.\nInje earned his B.S. degree in Electronic \u0026amp; Electrical Engineering at Hongik University. Before entering Korea University, he worked as a undergraduate researcher for two years at Hongik University.\n","date":1639958400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1639958400,"objectID":"8f59d9d90b034fab23c0cabdd2e60d51","permalink":"https://ku-csarch.github.io/authors/inje-kim/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/inje-kim/","section":"authors","summary":"Inje Kim is a graduate student pursuing a master’s degree in the Department of Computer Science and Engineering at Korea University. His research interest is in accelerator architecture, GPGPU, graph neural networks (GNNs).","tags":null,"title":"Inje Kim","type":"authors"},{"authors":["jonghyun-jeong"],"categories":null,"content":"Jonghyun Jeong is a graduate student pursuing a master’s degree in the Department of Computer Science and Engineering at Korea University. His research interest is in GPGPU. His current research focuses on memory systems of GPU.\nJonghyun earned his B.S. degree in Electronic and Electrical Engineering and Computer Engineering (Double Major) at Hongik University, South Korea. Before entering Korea University, he worked as a undergraduate researcher for 1.5 years at Hongik University.\n","date":1639958400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1639958400,"objectID":"f30b70d895d38eba84113ce1ef78e1ad","permalink":"https://ku-csarch.github.io/authors/jonghyun-jeong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jonghyun-jeong/","section":"authors","summary":"Jonghyun Jeong is a graduate student pursuing a master’s degree in the Department of Computer Science and Engineering at Korea University. His research interest is in GPGPU. His current research focuses on memory systems of GPU.","tags":null,"title":"Jonghyun Jeong","type":"authors"},{"authors":["seungho-jung"],"categories":null,"content":"Seungho Jung is a graduate student pursuing a master’s degree in the Department of Computer Science and Engineering at Korea University supervised by Professor Gunjae Koo. His research interests lie in GPU architecture, memory systems (especially HBMs), storage systems, and secure architecture. His current research focuses on secure GPU architecture on specific application.\nHe completed his B.S. in the Electronic \u0026amp; Electrical Engineering at the Hongik University.\n","date":1639958400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1639958400,"objectID":"82253009e81678bf573cd7ea03fd7ac4","permalink":"https://ku-csarch.github.io/authors/seungho-jung/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/seungho-jung/","section":"authors","summary":"Seungho Jung is a graduate student pursuing a master’s degree in the Department of Computer Science and Engineering at Korea University supervised by Professor Gunjae Koo. His research interests lie in GPU architecture, memory systems (especially HBMs), storage systems, and secure architecture.","tags":null,"title":"Seungho Jung","type":"authors"},{"authors":["hungjong-lee"],"categories":null,"content":"Hunjong Lee is a graduate student pursuing a master’s degree (PhD/Master integrated course) in the Department of Computer Science and Engineering at Korea University. His research interests lie in computer architecture and accelerator design. His current research focuses on DNN accelerator architecture.\nHunjong earned his B.S. in Electronic \u0026amp; Electrical Engineering at Hongik University, South Korea.\n","date":1597968000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1597968000,"objectID":"40cd99038d5367cf60fc8c8dd80fed80","permalink":"https://ku-csarch.github.io/authors/hunjong-lee/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/hunjong-lee/","section":"authors","summary":"Hunjong Lee is a graduate student pursuing a master’s degree (PhD/Master integrated course) in the Department of Computer Science and Engineering at Korea University. His research interests lie in computer architecture and accelerator design.","tags":null,"title":"Hunjong Lee","type":"authors"},{"authors":["boyoung-park"],"categories":null,"content":"Boyoung Park is a graduate student pursuing a master’s degree in the Department of Computer Science and Engineering at Korea University. Her research interest lies in near data processing and memory/storage systems. Her current research focuses on accelerating genomic analysis accommodating near data processing on storage platforms.\nBoyoung completed her B.S. in the Electronic \u0026amp; Electrical Engineering at Hongik University, South Korea.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8d78d4cab9e48201e9ffbdf0faed3afc","permalink":"https://ku-csarch.github.io/authors/boyoung-park/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/boyoung-park/","section":"authors","summary":"Boyoung Park is a graduate student pursuing a master’s degree in the Department of Computer Science and Engineering at Korea University. Her research interest lies in near data processing and memory/storage systems.","tags":null,"title":"Boyoung Park","type":"authors"},{"authors":["hyunwoo-moon"],"categories":null,"content":"Hyunwoo Moon is a graduate student pursuing a master’s degree in the Department of Computer Science and Engineering at Korea University. His research interests lie in memory systems and domain specific architectures.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5f1f0be50daba09c24550dd8a0f69585","permalink":"https://ku-csarch.github.io/authors/hyunwoo-moon/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/hyunwoo-moon/","section":"authors","summary":"Hyunwoo Moon is a graduate student pursuing a master’s degree in the Department of Computer Science and Engineering at Korea University. His research interests lie in memory systems and domain specific architectures.","tags":null,"title":"Hyunwoo Moon","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://ku-csarch.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Jongmin Lee","Junyeon Lee","Taeweon Suh","Gunjae Koo"],"categories":null,"content":"","date":1647388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647388800,"objectID":"91b47f73ea04fe266ac17a8f8d3c8103","permalink":"https://ku-csarch.github.io/publication/sec_victim_date22/","publishdate":"2021-11-25T00:00:00Z","relpermalink":"/publication/sec_victim_date22/","section":"publication","summary":"","tags":["Security","Secure Architecture","Transient Execution Attack","Cache Side-Channel"],"title":"CacheRewinder: Revoking Speculative Cache Updates Exploiting Write-Back Buffer","type":"publication"},{"authors":["Yeong Seo Lee","Gunjae Koo","Young-Ho Gong","Sung Woo Chung"],"categories":null,"content":"","date":1647388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647388800,"objectID":"c2cdbdff88e0f63cb5e0e3586eaa7b40","permalink":"https://ku-csarch.github.io/publication/dram_ecc_date22/","publishdate":"2021-11-25T00:00:00Z","relpermalink":"/publication/dram_ecc_date22/","section":"publication","summary":"","tags":["DRAM","Error Correction Code","Reliability","Chip Error Resilience"],"title":"Stealth ECC: A Data-Width Aware Adaptive ECC Scheme for DRAM Error Resilience","type":"publication"},{"authors":["Gunjae Koo","Yunho Oh","Hung-Wei Tseng","Won Woo Ro","Murali Annavaram"],"categories":null,"content":"","date":1645747200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645747200,"objectID":"cd21da754e61b09c8b397146076514c0","permalink":"https://ku-csarch.github.io/publication/ssd_index_tc22/","publishdate":"2022-02-25T00:00:00Z","relpermalink":"/publication/ssd_index_tc22/","section":"publication","summary":"Flash memory technologies rely on the flash translation layer (FTL) to manage no in-place update and garbage collection. Current FTL management schemes do not exploit the semantics of the accessed data. In this paper, we explore how semantic knowledge can be exploited to build and maintain indexes for stored data automatically. Data indexing is a critical enabler to accelerate many database applications and big data analytics. Unlike traditional per-table or per-file indexes that are managed separately from the data, we propose to maintain indexes on a per-flash page basis. Our approach, called FLash IndeXeR (FLIXR), builds and maintains page-level indexes whenever a page is written into the flash. FLIXR updates the indexes alongside any data updates at page granularity. The cost of the index update is hidden in the page write delays. FLIXR stores index data for each page within the FTL entry associated with that page, thereby piggybacking index access on a page access request. FLIXR accesses the index data in each FTL entry to determine whether the associated page stores data with a given key. FLIXR achieves 52.6% performance improvement for TPC-C and TPC-H benchmarks, compared to the conventional host-side indexing mechanism.","tags":["SSD","Database Index","In-Storage Processing"],"title":"FLIXR: Embedding Index into Flash Translation Layer in SSDs","type":"publication"},{"authors":["Gunjae Koo"],"categories":["Journal"],"content":"One paper was accepted to IEEE Transactions on Computers. Our paper proposes FLIXR, an efficient in-storage indexing mechanism in SSDs. FLIXR builds and maintains the page-level indexes exploiting the SSD’s native page translation structures. FLIXR automatically creates and updates new per-page indexes while new data is written to flash memory pages. FLIXR’s in-storage indexes are efficiently maintained in SSD’s flash translation layer alongside with page translation structures. Hence FLIXR can effectively reduce host processor’s heavy burdens for creating and maintaining index structures of large database structures.\n","date":1644710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644710400,"objectID":"7f690ed707b5850cb489e5ffc7812e9e","permalink":"https://ku-csarch.github.io/post/accepted-tc22/","publishdate":"2022-02-13T00:00:00Z","relpermalink":"/post/accepted-tc22/","section":"post","summary":"Our paper on an in-storage indexing mechanism was accepted to IEEE Transactions on Computers.","tags":["Accepted","IEEE TC","Journal"],"title":"One paper accepted to IEEE Transactions on Computers (Journal, SCIE)","type":"post"},{"authors":["Gunjae Koo","Jongmin Lee"],"categories":["Conference"],"content":"Jongmin presented his research work regarding the new cache side-channel attack mechanism that exploits the vulnerabilities in the undo-based archtiectural defense solutions at the 36th International Conference on Information Networking (ICOIN 2022) held in Jeju-si, Jeju-do, South Kroea. Good job, Jongmin!\n","date":1641945600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641945600,"objectID":"b0207e8c7b7e8d021519620a47809061","permalink":"https://ku-csarch.github.io/post/present-icoin22/","publishdate":"2022-01-12T00:00:00Z","relpermalink":"/post/present-icoin22/","section":"post","summary":"Jongmin presented his research on security attacks on the modern processors at the 36th International Conference on Information Networking (ICOIN) 2022.","tags":["Talk","Conference"],"title":"Jongmin presented at ICOIN 2022","type":"post"},{"authors":["Jongmin Lee","Gunjae Koo"],"categories":null,"content":"","date":1641945600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641945600,"objectID":"d316b6addd90be608ee065842b31d648","permalink":"https://ku-csarch.github.io/publication/sec_attack_icoin22/","publishdate":"2021-12-07T00:00:00Z","relpermalink":"/publication/sec_attack_icoin22/","section":"publication","summary":"","tags":["Security","Secure Architecture","Transient Execution Attack","Cache Side-Channel"],"title":"Restore Buffer Overflow Attacks: Breaking Undo-Based Defense Schemes","type":"publication"},{"authors":["Seungho Jung","Myung Kuk Yoon","Gunjae Koo"],"categories":null,"content":"","date":1639958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639958400,"objectID":"0491d9091d005e20b7832e401100ece7","permalink":"https://ku-csarch.github.io/publication/gpu_side_ksc21/","publishdate":"2021-11-25T00:00:00Z","relpermalink":"/publication/gpu_side_ksc21/","section":"publication","summary":"","tags":["GPU","Security","Side-Channel"],"title":"Analyzing Characteristics of Memory Timing Side-Channels in GPU","type":"publication"},{"authors":["Jonghyun Jeong","Yunho Oh","Gunjae Koo"],"categories":null,"content":"","date":1639958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639958400,"objectID":"91ae630b939543972989e9bd17e0f3cc","permalink":"https://ku-csarch.github.io/publication/gpu_cache_ksc21/","publishdate":"2021-11-25T00:00:00Z","relpermalink":"/publication/gpu_cache_ksc21/","section":"publication","summary":"Graphics processing unit (GPU)는 그래픽 어플리케이션의 처리 뿐만 아니라 최근 machine learning, big data analytics 등의 대규모 병렬처리를 요구하는 어플리케이션의 처리에 널리 사용되고 있다. GPU는 많은 수의 쓰레드를 동시에 실행하는 병렬처리 구조를 가지고 있다. 이 때문에 많은 메모리 요청이 단시간 내에 발생되어 메모리 계층구조의 자원이 소진되고 데이터 캐시가 비효율적으로 사용되는 문제가 있었다. 최신 GPU 아키텍처에서는 이러한 문제를 해결하기 위해 streaming cache라고 불리는 새로운 캐시 구조를 적용하여 데이터 캐시의 성능 저하 요소를 줄였다. 본 연구에서는 최신 GPU 시뮬레이터를 사용하여 streaming cache의 성능을 기존 캐시와 자세히 비교하여 streaming cache의 특성을 밝히고 있다. 본 연구에서는 streaming cache가 데이터 캐시의 congestion은 해결하지만 memory congestion이 interconnection network 단으로 옮겨갈 수 있음을 밝혀냈다. 본 연구를 통하여 streaming cache가 최신 GPU 메모리 시스템에서 미치는 영향을 분석하여 최신 GPU 아키텍처의 메모리 시스템에서 발생할 수 있는 성능 문제점들에 대해서 제시한다.","tags":["GPU","Cache","Characteristics"],"title":"Analyzing Data Cache Performance of Modern GPU Architecture","type":"publication"},{"authors":["Inje Kim","Gunjae Koo"],"categories":null,"content":"","date":1639958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639958400,"objectID":"10e1e328f8f3d8c5c84ee81b359b403b","permalink":"https://ku-csarch.github.io/publication/gnn_profile_ksc21/","publishdate":"2021-11-25T00:00:00Z","relpermalink":"/publication/gnn_profile_ksc21/","section":"publication","summary":"","tags":["GNN","Profiling","GPU"],"title":"Analyzing the Performance of GCN Inferences with respect to Sparsity of Graph Features","type":"publication"},{"authors":["Gunjae Koo","Inje Kim","Jonghyun Jeong","Seungho Jung"],"categories":["Conference (Korea)"],"content":"Inje, Jonghyun, and Seungho presented their research work at Korea Software Congress 2021 (KSC 2021) held in Pyeongchang-gun, Gangwon-do. Inje presented his research on GCN inference performance with respect to graph features. Jonghyun presented his research on data cache of modern GPU architecture. Seungho presented his research on characteristics of GPU memory side-channels. Our lab members also attended the conference to have a great time. Good job, guys!\n","date":1639958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639958400,"objectID":"92b2e4082405e155352e59fa001c74a1","permalink":"https://ku-csarch.github.io/post/present-ksc21/","publishdate":"2021-12-20T00:00:00Z","relpermalink":"/post/present-ksc21/","section":"post","summary":"Inje, Jonghyun, and Seungho presented their research work at Korea Software Congress (KSC) 2021.","tags":["Talk","Conference (Korea)"],"title":"Inje, Jonghyun, and Seungho presented at KSC 2021","type":"post"},{"authors":["Gunjae Koo"],"categories":["Service"],"content":"Prof. Gunjae Koo will be serving on the program committee for the 14th Workshop on the General Purpose Processing Using GPU (GPGPU 2022). GPGPU 2022 will be held in conjunction with HPCA 2022 in Seoul, South Korea. Please submit your best work!\n","date":1639958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639958400,"objectID":"cb8cd728fdf77d32a92fcdd0f79c8088","permalink":"https://ku-csarch.github.io/post/committee-gpgpu22/","publishdate":"2021-12-20T00:00:00Z","relpermalink":"/post/committee-gpgpu22/","section":"post","summary":"Prof. Gunjae Koo will be serving on the program committee for GPGPU 2022. Please submit your best work!","tags":["Committee","Workshop"],"title":"Prof. Gunjae Koo will be serving on the program committee for GPGPU 2022","type":"post"},{"authors":["Gunjae Koo","Jongmin Lee","Junyeon Lee","Taeweon Suh"],"categories":null,"content":"","date":1639526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639526400,"objectID":"bf4a12e28a37496e56051acdc721ec06","permalink":"https://ku-csarch.github.io/publication/sec_victim_patent21/","publishdate":"2021-12-15T00:00:00Z","relpermalink":"/publication/sec_victim_patent21/","section":"publication","summary":"","tags":["Security","Secure Architecture"],"title":"Processor and Operation Thereof to Revoke Cache Memory States Utilizing Write-Back Buffer","type":"publication"},{"authors":["Gunjae Koo","Jongmin Lee"],"categories":["Workshop"],"content":"One paper was accepted to Workshop on Information System Security (WISS) which will be held in conjunction with International Conference on Information Networking (ICOIN) 2022. Our work reveals the undo-based protection schemes are still vulnerable to the modified Prime+Probe type attack that makes the restore buffer overflowed. Using the proposed restore buffer overflow attack approach, we reveal the attack can leak part of secret data successfully even though the processor is protected by the undo-based defense scheme.\n","date":1638835200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638835200,"objectID":"6160fedfc62d3d686b6fd793e3e8898d","permalink":"https://ku-csarch.github.io/post/accepted-icoin22/","publishdate":"2021-12-07T00:00:00Z","relpermalink":"/post/accepted-icoin22/","section":"post","summary":"Our paper on architectural vulnerability in undo-type defense mechanisms was accepted to ICOIN 2022. (Congrats Jongmin!)","tags":["Accepted","ICOIN","Conference","Workshop"],"title":"One paper accepted to ICOIN 2022","type":"post"},{"authors":["Gunjae Koo"],"categories":["Funding"],"content":"Computer System Architecture (CSArch) Lab received the Korea University Insung Research Grant to support the research on near data processing for genome analytics. We want to express our sincere gratitude to generous supporters who have contributed to Korea University.\n","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"7fde4e0b0e2a8bceea4fb3d0fc05c41c","permalink":"https://ku-csarch.github.io/post/grant-insung21/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/post/grant-insung21/","section":"post","summary":"Our lab received funding from Korea University to support the research on near data processing for genome analytics.","tags":["Funding","Grant"],"title":"Received the Korea University Insung Research Grant","type":"post"},{"authors":["Gunjae Koo","Jongmin Lee"],"categories":["Conference"],"content":"Two papers were accepted to Design, Automation and Test in Europe Conference (DATE) 2022. One paper proposes CacheRewinder, an efficient architectural defense mechanism against transient execution attacks. CacheRewinder exploits the underutilized write-back buffer entries to revoke cache updates done by speculative executions. Another paper presents Stealth ECC, a cost-effective and strong memory protection scheme. Stealth ECC exploits not-meaningful data fields to provide stronger error correction capability.\n","date":1636588800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636588800,"objectID":"6c0d2aebdab90ea1ea6e8261995b4cd5","permalink":"https://ku-csarch.github.io/post/accepted-date22/","publishdate":"2021-11-11T00:00:00Z","relpermalink":"/post/accepted-date22/","section":"post","summary":"Our paper on secure architecture against transient execution attacks was accepted to DATE 2022. (Congrats Jongmin!) Another paper on an efficient DRAM ECC scheme was also accepted. (Congrats Young Seo!)","tags":["Accepted","DATE","Conference"],"title":"Two papers accepted to DATE 2022","type":"post"},{"authors":["Minkyu Song","Junyeon Lee","Taeweon Suh","Gunjae Koo"],"categories":null,"content":"","date":1636502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636502400,"objectID":"23493c2333e9128b45959d57d363f794","permalink":"https://ku-csarch.github.io/publication/sec_cache_mdpi21/","publishdate":"2021-11-10T00:00:00Z","relpermalink":"/publication/sec_cache_mdpi21/","section":"publication","summary":"Since cache side-channel attacks have been serious security threats to multi-tenant systems, there have been several studies to protect systems against the attacks. However, the prior studies have limitations in determining only the existence of the attack and/or occupying too many computing resources in runtime. We propose a low-overhead pinpointing solution, called RT-Sniper, to overcome such limitations. RT-Sniper employs a two-level filtering mechanism to minimize performance overhead. It first monitors hardware events per core and isolates a suspected core to run a malicious process. Then among the processes running on the selected core, RT-Sniper pinpoints a malicious process through a per-process monitoring approach. With the core-level filtering, RT-Sniper has an advantage in overhead compared to the previous works. We evaluate RT-Sniper against Flush+Reload and Prime+Probe attacks running SPEC2017, LMBench, and PARSEC benchmarks on multi-core systems. Our evaluation demonstrates that the performance overhead by RT-Sniper is negligible (0.3% for single-threaded applications and 2.05% for multi-threaded applications). Compared to the previous defense solutions against cache side-channel attacks, RT-Sniper exhibits better detection performance with lower performance overhead.","tags":["Security","Malware Detection","Cache Side-Channel Attack","Low-Overhead"],"title":"RT-Sniper: A Low-Overhead Defense Mechanism Pinpointing Cache Side-Channel Attacks","type":"publication"},{"authors":["Gunjae Koo"],"categories":["Journal"],"content":"One paper was accepted to MDPI Electronics. Our paper proposes RT-Sniper, a software-based lightweight defense solution against cache side-channel attacks. RT-Sniper pinpoints attack processes efficiently using a two-phased monitoring mechanism: core-level and process-level profiling. Hence, RT-Sniper can effectively detect malicious processes with negligible performance overhead.\n","date":1636156800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636156800,"objectID":"02e89bff48fc817d109d6e46a5c0b9b0","permalink":"https://ku-csarch.github.io/post/accepted-mdpi21/","publishdate":"2021-11-06T00:00:00Z","relpermalink":"/post/accepted-mdpi21/","section":"post","summary":"Our paper on a software-based defense solution against cache side-channel attacks was accepted to MDPI Electronics. (Congrats Minkyu and Junyeon!)","tags":["Accepted","MDPI Electronics","Journal"],"title":"One paper accepted to MDPI Electronics (Journal, SCIE)","type":"post"},{"authors":["Gunjae Koo"],"categories":["Funding"],"content":"Computer System Architecture (CSArch) Lab was awarded the innovative research lab initiation grant (최초혁신실험실) from National Research Foundation (NRF). The grant will allow us to set up fundamental research equipments for research on processor and memory system architecture. The amount of the grant is approximately $100,000. Thanks NRF!\n","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625097600,"objectID":"33b44040598aa0eeec037843f3ef14c7","permalink":"https://ku-csarch.github.io/post/award-nrf21/","publishdate":"2021-07-01T00:00:00Z","relpermalink":"/post/award-nrf21/","section":"post","summary":"Our lab was awarded the innovative research lab initiation grant (최초혁신실험실) from National Research Foundation (NRF).","tags":["Funding","Award"],"title":"Awarded the innovative research lab initiation grant (최초혁신실험실) from NRF","type":"post"},{"authors":["Gunjae Koo","Inje Kim"],"categories":["Conference (Korea)"],"content":"Inje presented his research on the characteristics of GCN inference models at Korea Computer Congress 2021 (KCC 2021) held in Seoguipo-si, Jeju-do. Our lab members also attended the conference to have a great time. Good job, Inje!\n","date":1624579200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624579200,"objectID":"4f5352cde1b37799dd5451eb86be806a","permalink":"https://ku-csarch.github.io/post/present-kcc21/","publishdate":"2021-06-25T00:00:00Z","relpermalink":"/post/present-kcc21/","section":"post","summary":"Inje presented his research on the characteristics of GCN inference models at Korea Computer Congress (KCC) 2021.","tags":["Talk","Conference (Korea)"],"title":"Inje presented at KCC 2021","type":"post"},{"authors":["Inje Kim","Gunjae Koo"],"categories":null,"content":"","date":1624579200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624579200,"objectID":"5e90c8f60397bfcb33d322a2c8fc45e1","permalink":"https://ku-csarch.github.io/publication/gnn_profile_kcc21/","publishdate":"2021-06-25T00:00:00Z","relpermalink":"/publication/gnn_profile_kcc21/","section":"publication","summary":"Graph Convolutional Neural Network (GCN)은 그래프 구조를 이용한 인공 신경망 (Graph Neural Network, GNN) 중의 하나로서 소셜 네트워크 분석 및 소비자 성향 분석, 추천 시스템 등의 여러 응용 분야에 이용될 수 있다. GCN은 비유클리드형 자료 구조의 형태를 가지는 그래프 구조에 대한 데이터 처리를 요구하기 때문에 기존의 Deep Neural Network (DNN)에 사용되는 데이터와는 차이점을 가지고 있다. 그렇기 때문에, GCN을 기존의 DNN을 처리하는 데에 주로 쓰이는 하드웨어 구조에서 실행했을 때의 특성을 분석하는 것은 향후 효과적인 GCN용 알고리즘 및 하드웨어를 설계하는 데에 있어서 필수적이다. 본 연구에서는 인공 신경망 구조를 처리하는 데에 주로 쓰이는 GPU에서 여러가지 GCN 추론 알고리즘을 실행하고 이를 GPU 프로파일러로 분석하여 해당 하드웨어 구조에서 GCN 추론 커널이 가지는 특성을 밝히고 있다. 본 연구에서는 GCN 추론 과정에 쓰이는 커널들이 크게 두 종류의 큰 차이를 보이는 특성을 보이는 커널들로 분류할 수 있음을 밝히고 있으며, 이러한 특성에 기반하여 GCN의 커널들이 GPU에서 실행 엔진과 캐시 메모리와 같은 하드웨어 자원을 비효율적으로 사용하고 있음을 밝혀내었다. 본 연구를 통하여 GCN의 최적화 방법 및 GCN을 효율적으로 실행하기 위한 구조적인 접근 방법에 대해서 도움을 줄 수 있다.","tags":["GNN","Profiling","GPU"],"title":"Revealing Characteristics of GCN Inference Models Using a GPU Profiler","type":"publication"},{"authors":["Gunjae Koo"],"categories":["Funding"],"content":"Computer System Architecture (CSArch) Lab received funding from National Research Foundation (NRF) to support the research on the processor and memory architecture for neural networks using large graph structures. Prof. Gunjae Koo was awarded the outstanding young scientist grant (우수신진연구) which is a part of Basic Research Projects funded by Korean government. Thanks NRF!\n","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"3fe0e74b9aa29558523df71105f035c3","permalink":"https://ku-csarch.github.io/post/grant-nrf21/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/post/grant-nrf21/","section":"post","summary":"Our lab received funding from National Research Foundation (NRF) to support the research on processor and memory architecture for neural networks using large graph structures.","tags":["Funding","Grant"],"title":"Received the outstanding young scientist grant (우수신진연구) from NRF","type":"post"},{"authors":["Murali Annavaram","Gunjae Koo","Kiran Kumar Matam","Hung-Wei Tseng"],"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"f4f9573d803f140024d1a7e7596f163b","permalink":"https://ku-csarch.github.io/publication/ssd_proc_patent20/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/publication/ssd_proc_patent20/","section":"publication","summary":"Publication of US20200310690A1","tags":["SSD","Near Data Processing","Dynamic Workload Offloading","Storage Systems"],"title":"Dynamic Near-Data Processing Control Mechanism Based on Computer Resource Availability on Solid-State Disk Platforms","type":"publication"},{"authors":["Hunjong Lee","Junhwan Yoo","Gunjae Koo"],"categories":null,"content":"","date":1597968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597968000,"objectID":"42a801d3bf09c312f05e61dd04ecf940","permalink":"https://ku-csarch.github.io/publication/accel_speaker_ieie20/","publishdate":"2020-08-21T00:00:00Z","relpermalink":"/publication/accel_speaker_ieie20/","section":"publication","summary":"Response time is one of the critical performance factors of artificial intelligence (AI) speakers. The internet network delays and the processing time on cloud server infrastructure dominate the response delays of AI speakers. The network delay is proportional to the size of packets that include the recorded queries. Normally this recorded sound data is not compressed since compression processes can be a heavy burden for the wimpy processors embedded in AI speakers. In this work we design an audio compression accelerator which can reduce the packet size of user queries. We implement the proposed accelerator on the FPGA-based SoC development board. Our evaluation reveals that the overall response time of an AI speaker is effectively reduced with the audio compression accelerator.","tags":["Accelerator","Compression","FPGA"],"title":"Audio Compression Accelerator Design for Improving the Response Time of AI Speakers","type":"publication"},{"authors":["Won Jeon","Jun Hyun Park","Yoonsoo Kim","Gunjae Koo","Won Woo Ro"],"categories":null,"content":"# #Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.\n    #Supplementary notes can be added here, including code and math.\n","date":1594598400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594598400,"objectID":"765c916eb372c35c663de792f60aaa07","permalink":"https://ku-csarch.github.io/publication/gpu_nvrf_access20/","publishdate":"2020-07-13T00:00:00Z","relpermalink":"/publication/gpu_nvrf_access20/","section":"publication","summary":"Modern Graphics Processing Units (GPUs) require large hardware resources for massive parallel thread executions. In particular, modern GPUs have a large register file composed of Static Random Access Memory (SRAM). Due to the high leakage current of SRAM, the register file consumes approximately 20% of the total GPU energy. The energy efficiency of the register file becomes more critical as the throughput of GPUs increases. For more energy-efficient GPUs, the usage of non-volatile memory such as Spin-Transfer Torque Magnetic Random Access Memory (STT-MRAM) as the GPU register file has been studied extensively. STT-MRAM requires a lower leakage current compared to SRAM and provides an appropriate read performance. However, using STT-MRAM directly in the GPU register file causes problems in performance and endurance because of complicated write procedures and material characteristics. To overcome these challenges, we propose a novel register file architecture and its management system for GPUs, named Hi-End, which exploits the data locality and compressibility of the register file. For STT-MRAM-based GPU register files, Hi-End increases the data write performance and endurance by caching and data compression, respectively. In our evaluation, Hi-End enhances the energy efficiency of a GPU register file by 70.02% and reduces the write operations by up to 95.98% with negligible performance degradation compared to SRAM-based register files.","tags":["GPU","Register File","STT-MRAM","Energy Efficiency","Data Compression"],"title":"Hi-End: Hierarchical, Endurance-Aware STT-MRAM-Based Register File for Energy-Efficient GPUs","type":"publication"},{"authors":["Kiran Kumar Matam","Gunjae Koo","Haipeng Zha","Hung-Wei Tseng","Murali Annavaram"],"categories":null,"content":"","date":1561507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561507200,"objectID":"ec8af1541110f5aaf6d976193a2883b6","permalink":"https://ku-csarch.github.io/publication/ssd_graph_isca19/","publishdate":"2019-06-26T00:00:00Z","relpermalink":"/publication/ssd_graph_isca19/","section":"publication","summary":"Graph analytics play a key role in a number of applications such as social networks, drug discovery, and recommendation systems. Given the large size of graphs that may exceed the capacity of the main memory, application performance is bounded by storage access time. Out-of-core graph processing frameworks try to tackle this storage access bottleneck through techniques such as graph sharding, and sub-graph partitioning. Even with these techniques, the need to access data across different graph shards or sub-graphs causes storage systems to become a significant performance hurdle. In this paper, we propose a graph semantic aware solid state drive (SSD) framework, called GraphSSD, which is a full system solution for storing, accessing, and performing graph analytics on SSDs. Rather than treating storage as a collection of blocks, GraphSSD considers graph structure while deciding on graph layout, access, and update mechanisms. GraphSSD replaces the conventional logical to physical page mapping mechanism in an SSD with a novel vertex-to-page mapping scheme and exploits the detailed knowledge of the flash properties to minimize page accesses. GraphSSD also supports efficient graph updates (vertex and edge modifications) by minimizing unnecessary page movement overheads. GraphSSD provides a simple programming interface that enables application developers to access graphs as native data in their applications, thereby simplifying the code development. It also augments the NVMe (non-volatile memory express) interface with a minimal set of changes to map the graph access APIs to appropriate storage access mechanisms. Our evaluation results show that the GraphSSD framework improves the performance by up to 1.85 × for the basic graph data fetch functions and on average 1.40×, 1.42×, 1.60×, 1.56×, and 1.29× for the widely used breadth-first search, connected components, random-walk, maximal independent set, and page rank applications, respectively.","tags":["SSD","Storage","Graphs"],"title":"GraphSSD: Graph Semantics Aware SSD","type":"publication"},{"authors":["Yunho Oh","Gunjae Koo","Murali Annavaram","Won Woo Ro"],"categories":null,"content":"","date":1561507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561507200,"objectID":"76ff69ce7d7740542ffe998f1d3cd677","permalink":"https://ku-csarch.github.io/publication/gpu_lb_isca19/","publishdate":"2019-06-26T00:00:00Z","relpermalink":"/publication/gpu_lb_isca19/","section":"publication","summary":"Modern GPUs suffer from cache contention due to the limited cache size that is shared across tens of concurrently running warps. To increase the per-warp cache size prior techniques proposed warp throttling which limits the number of active warps. Warp throttling leaves several registers to be dynamically unused whenever a warp is throttled. Given the stringent cache size limitation in GPUs this work proposes a new cache management technique named Linebacker (LB) that improves GPU performance by utilizing idle register file space as victim cache space. Whenever a CTA becomes inactive, linebacker backs up the registers of the throttled CTA to the off-chip memory. Then, linebacker utilizes the corresponding register file space as victim cache space. If any load instruction finds data in the victim cache line, the data is directly copied to the destination register through a simple register-register move operation. To further improve the efficiency of victim cache linebacker allocates victim cache space only to a select few load instructions that exhibit high data locality. Through a careful design of victim cache indexing and management scheme linebacker provides 29.0% of speedup compared to the previously proposed warp throttling techniques.","tags":["GPU","Cache","Register File","Scheduling"],"title":"Linebacker: Preserving Victim Cache Lines in Idle Register Files of GPUs","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three   A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}   Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://ku-csarch.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Gunjae Koo","Vivek Kozhikkottu","Shankar Ganesh Ramasubramanian","Christopher B. Wilkerson"],"categories":null,"content":"","date":1530748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530748800,"objectID":"23df7bd59ce22924ba0584ebd4faa8e3","permalink":"https://ku-csarch.github.io/publication/dram_ctrl_patent18/","publishdate":"2018-07-05T00:00:00Z","relpermalink":"/publication/dram_ctrl_patent18/","section":"publication","summary":"Publication of US20180188976A1","tags":["DRAM","Memory Controller"],"title":"Increasing Read Pending Queue Capacity to Increase Memory Bandwidth","type":"publication"},{"authors":["Gunjae Koo","Hyeran Jeon","Zhenhong Liu","Nam Sung Kim","Murali Annavaram"],"categories":null,"content":"","date":1527206400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527206400,"objectID":"e845389df646b79218cb072a93488682","permalink":"https://ku-csarch.github.io/publication/gpu_caps_ipdps18/","publishdate":"2018-05-25T00:00:00Z","relpermalink":"/publication/gpu_caps_ipdps18/","section":"publication","summary":"Albeit GPUs are supposed to be tolerant to long latency of data fetch operation, we observe that L1 cache misses occur in a bursty manner for many memory-intensive applications. This in turn leads to severe contentions in GPU memory hierarchy, and thus stalls execution pipeline for many cycles as all warps end up waiting for their memory requests to be serviced by L1 cache. To spread such bursty L1 cache misses, we propose CTA-Aware Prefetcher and Scheduler (CAPS) consisting of a thread group-aware prefetcher and a prefetch-aware warp scheduler for GPUs. GPU kernels group threads into cooperative thread arrays (CTAs). Each thread typically uses its thread index and its associated CTA index to identify the data that it operates on. The starting base address accessed by the first warp in a CTA is difficult to predict since that starting address is a complex function of thread index and CTA index and also depends on how the programmer distributes input data across CTAs. But threads within each CTA exhibit stride accesses. Hence, if the base address of each CTA can be computed early, it is possible to accurately predict prefetch addresses for threads within a CTA. To compute the base address of each CTA, a leading warp is used from each CTA. The leading warp is executed early by pairing it with warps from currently executing leading CTA. The warps in the leading CTA are used to compute the stride value. The stride value is then combined with base addresses computed from the leading warp of each CTA to prefetch the data for all the trailing warps in the trailing CTAs. CAPS allows prefetch requests to be issued sufficiently ahead of time before the demand requests, effectively reorganizing warp executions to quickly detect the base address of each CTA and stride per load. CAPS predicts addresses with over 97% accuracy and is able to improve GPU performance by 8% on average with up to 27% for a wide range of GPU applications.","tags":["GPU","Prefetch","SIMT","Scheduling"],"title":"CTA-Aware Prefetching and Scheduling for GPU","type":"publication"},{"authors":["Gunjae Koo","Kiran Kumar Matam","Te I","H. V. Krishna Giri Narra","Jing Li","Hung-Wei Tseng","Steven Swanson","Murali Annavaram"],"categories":null,"content":"","date":1507939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507939200,"objectID":"09aafa67eeef59c83f6c202da80779f9","permalink":"https://ku-csarch.github.io/publication/ssd_proc_micro17/","publishdate":"2017-10-14T00:00:00Z","relpermalink":"/publication/ssd_proc_micro17/","section":"publication","summary":"Modern data center solid state drives (SSDs) integrate multiple general-purpose embedded cores to manage flash translation layer, garbage collection, wear-leveling, and etc., to improve the performance and the reliability of SSDs. As the performance of these cores steadily improves there are opportunities to repurpose these cores to perform application driven computations on stored data, with the aim of reducing the communication between the host processor and the SSD. Reducing host-SSD bandwidth demand cuts down the I/O time which is a bottleneck for many applications operating on large data sets. However, the embedded core performance is still significantly lower than the host processor, as generally wimpy embedded cores are used within SSD for cost effective reasons. So there is a trade-off between the computation overhead associated with near SSD processing and the reduction in communication overhead to the host system. In this work, we design a set of application programming interfaces (APIs) that can be used by the host application to offload a data intensive task to the SSD processor. We describe how these APIs can be implemented by simple modifications to the existing Non-Volatile Memory Express (NVMe) command interface between the host and the SSD processor. We then quantify the computation versus communication tradeoffs for near storage computing using applications from two important domains, namely data analytics and data integration. Using a fully functional SSD evaluation platform we perform design space exploration of our proposed approach by varying the bandwidth and computation capabilities of the SSD processor. We evaluate static and dynamic approaches for dividing the work between the host and SSD processor, and show that our design may improve the performance by up to 20% when compared to processing at the host processor only, and 6X when compared to processing at the SSD processor only.","tags":["SSD","Near Data Processing","Dynamic Workload Offloading","Storage Systems"],"title":"Summarizer: Trading Communication with Computing near Storage","type":"publication"},{"authors":["Gunjae Koo","Yunho Oh","Won Woo Ro","Murali Annavaram"],"categories":null,"content":"","date":1498262400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498262400,"objectID":"7a907e2f414f462709e2fb7e9b5b5b25","permalink":"https://ku-csarch.github.io/publication/gpu_apcm_isca17/","publishdate":"2017-06-24T00:00:00Z","relpermalink":"/publication/gpu_apcm_isca17/","section":"publication","summary":"Long latency of memory operation is a prominent performance bottleneck in graphics processing units (GPUs). The small data cache that must be shared across dozens of warps (a collection of threads) creates significant cache contention and premature data eviction. Prior works have recognized this problem and proposed warp throttling which reduces the number of active warps contending for cache space. In this paper we discover that individual load instructions in a warp exhibit four different types of data locality behavior: (1) data brought by a warp load instruction is used only once, which is classified as streaming data (2) data brought by a warp load is reused multiple times within the same warp, called intra-warp locality (3) data brought by a warp is reused multiple times but across different warps, called inter-warp locality (4) and some data exhibit both a mix of intra- and inter-warp locality. Furthermore, each load instruction exhibits consistently the same locality type across all warps within a GPU kernel. Based on this discovery we argue that cache management must be done using per-load locality type information, rather than applying warp-wide cache management policies. We propose Access Pattern-aware Cache Management (APCM), which dynamically detects the locality type of each load instruction by monitoring the accesses from one exemplary warp. APCM then uses the detected locality type to selectively apply cache bypassing and cache pinning of data based on load locality characterization. Using an extensive set of simulations we show that APCM improves performance of GPUs by 34% for cache sensitive applications while saving 27% of energy consumption over baseline GPU.","tags":["GPU","Cache","Access Pattern"],"title":"Access Pattern-Aware Cache Management for Improving Data Utilization in GPU","type":"publication"},{"authors":["Sangpil Lee","Keunsoo Kim","Gunjae Koo","Hyeran Jeon","Won Woo Ro","Murali Annavaram"],"categories":null,"content":"# #Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.\n    #Supplementary notes can be added here, including code and math.\n","date":1493596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493596800,"objectID":"30b783f252515ef46b96a430ade15c3a","permalink":"https://ku-csarch.github.io/publication/gpu_rfcomp_tc17/","publishdate":"2017-05-01T00:00:00Z","relpermalink":"/publication/gpu_rfcomp_tc17/","section":"publication","summary":"GPU design trends show that the register file size will continue to increase to enable even more thread level parallelism. As a result register file consumes a large fraction of the total GPU chip power. This paper explores register file data compression for GPUs to improve power efficiency. Compression reduces the width of the register file read and write operations, which in turn reduces dynamic power. This work is motivated by the observation that the register values of threads within the same warp are similar, namely the arithmetic differences between two successive thread registers is small. Compression exploits the value similarity by removing data redundancy of register values. Without decompressing operand values some instructions can be processed inside register file, which enables to further save energy by minimizing data movement and processing in power hungry main execution unit. Evaluation results show that the proposed techniques save 25 percent of the total register file energy consumption and 21 percent of the total execution unit energy consumption with negligible performance impact.","tags":["GPU","Register File","Energy Efficiency","Data Compression"],"title":"Improving Energy Efficiency of GPUs through Data Compression and Compressed Execution","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://ku-csarch.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":["Keunsoo Kim","Sangpil Lee","Myung Kuk Yoon","Gunjae Koo","Won Woo Ro","Murali Annavaram"],"categories":null,"content":"","date":1458086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1458086400,"objectID":"be7823eb0c03c5f8ac83ba7be459d739","permalink":"https://ku-csarch.github.io/publication/gpu_pwarp_hpca16/","publishdate":"2016-03-16T00:00:00Z","relpermalink":"/publication/gpu_pwarp_hpca16/","section":"publication","summary":"This paper presents a pre-execution approach for improving GPU performance, called P-mode (pre-execution mode). GPUs utilize a number of concurrent threads for hiding processing delay of operations. However, certain long-latency operations such as off-chip memory accesses often take hundreds of cycles and hence leads to stalls even in the presence of thread concurrency and fast thread switching capability. It is unclear if adding more threads can improve latency tolerance due to increased memory contention. Further, adding more threads increases on-chip storage demands. Instead we propose that when a warp is stalled on a long-latency operation it enters P-mode. In P-mode, a warp continues to fetch and decode successive instructions to identify any independent instruction that is not on the long latency dependence chain. These independent instructions are then pre-executed. To tackle write-after-write and write-after-read hazards, during P-mode output values are written to renamed physical registers. We exploit the register file underutilization to re-purpose a few unused registers to store the P-mode results. When a warp is switched from P-mode to normal execution mode it reuses pre-executed results by reading the renamed registers. Any global load operation in P-mode is transformed into a pre-load which fetches data into the L1 cache to reduce future memory access penalties. Our evaluation results show 23% performance improvement for memory intensive applications, without negatively impacting other application categories.","tags":["GPU","SIMT","Scheduling"],"title":"Warped-Preexecution: A GPU Pre-Execution Approach for Improving Latency Hiding","type":"publication"},{"authors":["Gunjae Koo","Hyeran Jeon","Murali Annavaram"],"categories":null,"content":"","date":1444089600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1444089600,"objectID":"d361d1c6b955a2287ef9a802c0490812","permalink":"https://ku-csarch.github.io/publication/gpu_load_iiswc15/","publishdate":"2015-10-06T00:00:00Z","relpermalink":"/publication/gpu_load_iiswc15/","section":"publication","summary":"In graphics processing units (GPUs), memory access latency is one of the most critical performance hurdles. Several warp schedulers and memory prefetching algorithms have been proposed to avoid the long memory access latency. Prior application characterization studies shed light on the interaction between applications, GPU micro architecture and memory subsystem behavior. Most of these studies, however, only present aggregate statistics on how memory system behaves over the entire application run. In particular, they do not consider how individual load instructions in a program contribute to the aggregate memory system behavior. The analysis presented in this paper shows that there are two distinct classes of load instructions, categorized as deterministic and non-deterministic loads. Using a combination of profiling data from a real GPU card and cycle accurate simulation data we show that there is a significant performance impact disparity when executing these two types of loads. We discuss and suggest several approaches to treat these two load categories differently within the GPU micro architecture for optimizing memory system performance.","tags":["GPU","Load","Cache","Scheduling"],"title":"Revealing Critical Loads and Hidden Data Locality in GPGPU Applications","type":"publication"},{"authors":["Sangpil Lee","Keunsoo Kim","Gunjae Koo","Hyeran Jeon","Won Woo Ro","Murali Annavaram"],"categories":null,"content":"","date":1434153600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1434153600,"objectID":"a02f42b3074871a4c97b3ab7947453da","permalink":"https://ku-csarch.github.io/publication/gpu_rfcomp_isca15/","publishdate":"2015-06-13T00:00:00Z","relpermalink":"/publication/gpu_rfcomp_isca15/","section":"publication","summary":"This paper presents Warped-Compression, a warp-level register compression scheme for reducing GPU power consumption. This work is motivated by the observation that the register values of threads within the same warp are similar, namely the arithmetic differences between two successive thread registers is small. Removing data redundancy of register values through register compression reduces the effective register width, thereby enabling power reduction opportunities. GPU register files are huge as they are necessary to keep concurrent execution contexts and to enable fast context switching. As a result register file consumes a large fraction of the total GPU chip power. GPU design trends show that the register file size will continue to increase to enable even more thread level parallelism. To reduce register file data redundancy warped-compression uses low-cost and implementation-efficient base-delta-immediate (BDI) compression scheme, that takes advantage of banked register file organization used in GPUs. Since threads within a warp write values with strong similarity, BDI can quickly compress and decompress by selecting either a single register, or one of the register banks, as the primary base and then computing delta values of all the other registers, or banks. Warped-compression can be used to reduce both dynamic and leakage power. By compressing register values, each warp-level register access activates fewer register banks, which leads to reduction in dynamic power. When fewer banks are used to store the register content, leakage power can be reduced by power gating the unused banks. Evaluation results show that register compression saves 25% of the total register file power consumption.","tags":["GPU","Register File","Data Compression","Energy Efficiency"],"title":"Warped-Compression: Enabling Power Efficient GPUs through Register Compression","type":"publication"},{"authors":["Gunjae Koo","Kyoung Won Lim","Seung Jong Choi"],"categories":null,"content":"","date":1294790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1294790400,"objectID":"dde6824d6d30706ce68b1b9bb97eb727","permalink":"https://ku-csarch.github.io/publication/frc_icce11/","publishdate":"2011-01-12T00:00:00Z","relpermalink":"/publication/frc_icce11/","section":"publication","summary":"In this paper, we present complementary motion estimation algorithm for motion compensated frame rate up-conversion. The proposed algorithm combines forward and backward motion estimation results to make up for the weakness of each motion estimation method. It also allocates true motion vectors in occlusion regions by using the temporal relations of the forward and backward motion estimation. Thus, we reduce artifacts by false motion vectors near occlusion regions in a compensated frame.","tags":["Image Processing","Motion Estimation","Frame Rate Conversion"],"title":"Complementary Block-Based Motion Estimation for Frame Rate Up-Conversion","type":"publication"},{"authors":["Gunjae Koo","Woochul Jung","Heesub Lee"],"categories":null,"content":"","date":1148428800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1148428800,"objectID":"0f7af2cebe0b90ecab9c7f6d32940c26","permalink":"https://ku-csarch.github.io/publication/prml_iscas06/","publishdate":"2006-05-24T00:00:00Z","relpermalink":"/publication/prml_iscas06/","section":"publication","summary":"In this paper, a PRML read channel that supports multiple optical disc formats, i.e. CD, DVD and BD is presented. The read channel includes digital timing recovery that generates timing matched data by interpolation, which can acquire high controllability and stability with small hardware. PRML bit detection is applied to the read channel in order to reduce bit errors for severe channel condition such as BD and high speed DVD. Also, PR-level of PRML is adaptively controlled to compensate asymmetry and signal level shift due to defects. To support high operating speed, the read channel is designed in a 2times-parallel processing. The read channel uses a 115 MHz main clock, and can support up to 8times DVD, equivalent to a channel rate of 210 MHz","tags":["PRML","Read Channel","Timing Recovery"],"title":"A Robust PRML Read Channel with Digital Timing Recovery for Multi-Format Optical Disc","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://ku-csarch.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://ku-csarch.github.io/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://ku-csarch.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]